{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Main Tasks Breakdown \n",
    "\n",
    "** Due Dates:**\n",
    "\n",
    "* Presentation (**Dec 15th - 19th**)\n",
    "* Report (**Dec 19th**)\n",
    "\n",
    "\n",
    "### 1. Implementation \n",
    "\n",
    "#### Paragraph to Vec (target: Nov 26th)\n",
    "\n",
    "* Finalize and its output need to align with the dimension of CNN output\n",
    "\n",
    "#### Word Embedding (target: Nov 26th)\n",
    "\n",
    "* Pretrained google word2vec\n",
    "\n",
    "* Not trained word embedding to be trained with CNN\n",
    "\n",
    "#### CNN (need to follow the paper) (target: Nov 26th)\n",
    "\n",
    "* Paper has a unique 2 hidden layer structure CNN output\n",
    "\n",
    "* Region sizes, filter size, width size to mirror the paper\n",
    "\n",
    "#### Open Classification\n",
    "\n",
    "* 1-vs-Rest to mirror paper (**target: Nov 26th**)\n",
    "\n",
    "* Clustering approach\n",
    "    * Gaussian Mixture Model (**target: Dec 2nd**)\n",
    "    * Infinite Dirichlet process (**target: Dec 9th**)\n",
    "\n",
    "### 2. Training and Cross Validation (Dec 14th)\n",
    "\n",
    "#### Full combinations: can start in pipeline or parallel once the implementation is ready\n",
    "\n",
    "* paragraph vec + 1-vs-rest\n",
    "* paragraph vec + GM\n",
    "* paragraph vec + IDPs\n",
    "* CNN + 1-vs-rest\n",
    "* CNN + GM\n",
    "* CNN + IDP\n",
    "\n",
    "### 3. Experiment Evaluation (target Dec 16th)\n",
    "\n",
    "* We need to carefully follow the metric used the in the paper **Section 3**\n",
    "* Ian mentioned this is a critical step\n",
    "\n",
    "### 4. Report write-up (target Dec 19th)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.  Project Progress Status\n",
    "\n",
    "### Week of Nov 19th\n",
    "\n",
    "* Understanding of open classificaiton concept (Qian, Raj, Leslie)\n",
    "* Exploration of simple models\n",
    "    * TF-IDF Model \n",
    "    * Paragraph2Vec\n",
    "    * BOW model\n",
    "* CNN code exploration \n",
    "* Midterm Report \n",
    "\n",
    "#### Issues:\n",
    "    * Cound not find meaningful data using simpler models\n",
    "    * 1-vs-rest test show small probability, need more understanding on how to set up threshold for open classification\n",
    "\n",
    "### Week of Nov 26th\n",
    "\n",
    "* Completed CNN implementation with softmax as output layer\n",
    "* Completed both to-train embedding layer and google word2vec pretrained embeddding layer to work with CNN\n",
    "\n",
    "#### Issues:\n",
    "    * Small machine crashes for even small document size when training with CNN\n",
    "\n",
    "\n",
    "### Week of Dec 3rd\n",
    "\n",
    "* 1-vs-Rest Classifiation approach\n",
    "* Add hidden layer to CNN to match DOC paper \n",
    "* Experiment with paragraph2Vec\n",
    "    * Setup stratify CV methods using paragraph2vec before using CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph Vector Model \n",
    "\n",
    "#### Ref: (Le and Mikolov, 2014) https://arxiv.org/pdf/1405.4053v2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN (Converlutional Neural Network)\n",
    "\n",
    "#### Ref:\n",
    "\n",
    "**Model Structure and Parameters:**\n",
    "\n",
    "* Embedding Layer:\n",
    "\n",
    "    * batch_size: N\n",
    "    * number of class: M\n",
    "    * word embedding dimension: d\n",
    "    * Sentence length: s\n",
    "    * A sentence matrix has size of $s \\times d$\n",
    "    \n",
    "    \n",
    "* CNN Layer\n",
    "\n",
    "    * Tensor flow cnn: _tf.nn.conv2d_\n",
    "    * Region size: h e.g. [3, 4, 5] has 3 regions (***Tensor flow define it as height***)\n",
    "    * Width of the filter: d (***Tensor flow define it as width, usually equal to word embedding dimension***)\n",
    "    * Number of filters per region: f (***This is equalt to number of feature maps for each region size***)\n",
    "    * 1 Max-pooling apply to 1 feature map\n",
    "    \n",
    "    Use the below image as an example\n",
    "    \n",
    "    If we use h = 3 region size [3, 4, 5], width of the filter is d = 5, and 2 filter for each region, we will have 2 feature maps for each regions size and total of 6 feature maps. Max polling will create 6 output vectors. We concatenate to form the feature vector for the output layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cnn.png\" width = \"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Output Layer: The DOC paper used 2 fully connected layer convert back to dimension M using $y = W'(ReLU(W o + b)) + b'$ where:\n",
    "    * o: feature vector from cnn layer after max-poolling is in $k$\n",
    "    * W: $r \\times k$\n",
    "    * W': $M \\times r$\n",
    "    * y: $M$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Open Classification\n",
    "\n",
    "### Approach to Implement\n",
    "\n",
    "* 1-vs-Rest Layer of DOC (This is the method of reference paper)\n",
    "\n",
    "    * M (number of class) sigmoid function, N (batch_size)\n",
    "    * Objective function for training is $$loss = \\sum_{i=1}^M \\sum_{i=1}^N y_n log(p) + (1 - y_n)log(1 - p(y))$$ is the summation of all log loss (cross-entropy) on the training data.\n",
    "    * At prediction, reject if all predicted probability is less than their threshold t_i, otherwise $argmax(Sigmoid(d))$\n",
    "    * The theshold is determined by using outlier detection. (***We can use a fixed number such as 0.95 to validate our model implementation***)\n",
    "\n",
    "\n",
    "* Clustering Approach (**NEW**)\n",
    "    * Gausian Mix Model\n",
    "    * Infinite Dirichlet process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. Test and Evaluation Methods and Metrics\n",
    "\n",
    "### Data Setup\n",
    "\n",
    "* Use 20 newsgroup data\n",
    "* The DOC paper use the top ** _20000_ ** Most frequent vocabulary. \n",
    "\n",
    "### Hyper Parameters to Tune\n",
    "\n",
    "* The DOC paper used the following hyper parameters\n",
    "    * CNN 3 filter size: [3, 4, 5]\n",
    "    * Document length: 2000\n",
    "    * 150 filter size\n",
    "    * 250 for the final hidden layer\n",
    "* We adjust the hyper parameter for the reason below\n",
    "    * Reduce document length based on length distribution\n",
    "    * Tuning the hidden layer size and filter size\n",
    "    * Maintain the CNN filter size\n",
    "    \n",
    "### Cross Validation\n",
    "\n",
    "* Random sample 60% document as training, 10% for validation, and 30% for testing\n",
    "* Vary number of training classes\n",
    "    * 25%, 50%, 75%, 100% => Based on workload\n",
    "    * Use Macro-F1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
